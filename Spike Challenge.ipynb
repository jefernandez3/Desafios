{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPIKE CHALLENGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"caudal_extra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'codigo_estacion', 'institucion', 'fuente', 'nombre',\n",
       "       'altura', 'latitud', 'longitud', 'codigo_cuenca', 'nombre_sub_cuenca',\n",
       "       'cantidad_observaciones', 'fecha', 'caudal', 'gauge_id', 'gauge_name',\n",
       "       'precip_promedio', 'temp_max_promedio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1411180 entries, 0 to 1411179\n",
      "Data columns (total 17 columns):\n",
      "Unnamed: 0                1411180 non-null int64\n",
      "codigo_estacion           1411180 non-null int64\n",
      "institucion               1411180 non-null object\n",
      "fuente                    1411180 non-null object\n",
      "nombre                    1411180 non-null object\n",
      "altura                    1411180 non-null int64\n",
      "latitud                   1411180 non-null float64\n",
      "longitud                  1411180 non-null float64\n",
      "codigo_cuenca             1411180 non-null int64\n",
      "nombre_sub_cuenca         1411180 non-null object\n",
      "cantidad_observaciones    1411180 non-null int64\n",
      "fecha                     1411180 non-null object\n",
      "caudal                    1411180 non-null float64\n",
      "gauge_id                  1411180 non-null int64\n",
      "gauge_name                1411180 non-null object\n",
      "precip_promedio           1383413 non-null float64\n",
      "temp_max_promedio         1259617 non-null float64\n",
      "dtypes: float64(5), int64(6), object(6)\n",
      "memory usage: 183.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELAMIENTO DESCRIPTIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>codigo_estacion</th>\n",
       "      <th>altura</th>\n",
       "      <th>latitud</th>\n",
       "      <th>longitud</th>\n",
       "      <th>codigo_cuenca</th>\n",
       "      <th>cantidad_observaciones</th>\n",
       "      <th>caudal</th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>precip_promedio</th>\n",
       "      <th>temp_max_promedio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.411180e+06</td>\n",
       "      <td>1.383413e+06</td>\n",
       "      <td>1.259617e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.055895e+05</td>\n",
       "      <td>6.279953e+06</td>\n",
       "      <td>5.855691e+02</td>\n",
       "      <td>-3.367748e+01</td>\n",
       "      <td>-7.121826e+01</td>\n",
       "      <td>6.255881e+01</td>\n",
       "      <td>1.573983e+04</td>\n",
       "      <td>9.552229e+01</td>\n",
       "      <td>6.279953e+06</td>\n",
       "      <td>1.921009e+00</td>\n",
       "      <td>1.921110e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.073727e+05</td>\n",
       "      <td>2.891695e+06</td>\n",
       "      <td>8.652682e+02</td>\n",
       "      <td>6.940820e+00</td>\n",
       "      <td>1.174363e+00</td>\n",
       "      <td>2.839921e+01</td>\n",
       "      <td>7.545197e+03</td>\n",
       "      <td>2.526337e+02</td>\n",
       "      <td>2.891695e+06</td>\n",
       "      <td>7.361936e+00</td>\n",
       "      <td>7.307081e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.020003e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.404110e+01</td>\n",
       "      <td>-7.328330e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>8.020000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.020003e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.905000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.527948e+05</td>\n",
       "      <td>4.320001e+06</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>-3.721190e+01</td>\n",
       "      <td>-7.206810e+01</td>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>1.025500e+04</td>\n",
       "      <td>1.250000e+00</td>\n",
       "      <td>4.320001e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.370000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.055895e+05</td>\n",
       "      <td>5.734001e+06</td>\n",
       "      <td>3.950000e+02</td>\n",
       "      <td>-3.359390e+01</td>\n",
       "      <td>-7.125470e+01</td>\n",
       "      <td>5.700000e+01</td>\n",
       "      <td>1.488900e+04</td>\n",
       "      <td>8.950000e+00</td>\n",
       "      <td>5.734001e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.908429e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.058384e+06</td>\n",
       "      <td>8.308000e+06</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>-2.994580e+01</td>\n",
       "      <td>-7.053280e+01</td>\n",
       "      <td>8.300000e+01</td>\n",
       "      <td>2.061100e+04</td>\n",
       "      <td>6.970000e+01</td>\n",
       "      <td>8.308000e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.520000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.411179e+06</td>\n",
       "      <td>1.287600e+07</td>\n",
       "      <td>4.370000e+03</td>\n",
       "      <td>-1.823250e+01</td>\n",
       "      <td>-6.814390e+01</td>\n",
       "      <td>1.280000e+02</td>\n",
       "      <td>3.666700e+04</td>\n",
       "      <td>1.580500e+04</td>\n",
       "      <td>1.287600e+07</td>\n",
       "      <td>2.586000e+02</td>\n",
       "      <td>4.150000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  codigo_estacion        altura       latitud  \\\n",
       "count  1.411180e+06     1.411180e+06  1.411180e+06  1.411180e+06   \n",
       "mean   7.055895e+05     6.279953e+06  5.855691e+02 -3.367748e+01   \n",
       "std    4.073727e+05     2.891695e+06  8.652682e+02  6.940820e+00   \n",
       "min    0.000000e+00     1.020003e+06  0.000000e+00 -5.404110e+01   \n",
       "25%    3.527948e+05     4.320001e+06  4.500000e+01 -3.721190e+01   \n",
       "50%    7.055895e+05     5.734001e+06  3.950000e+02 -3.359390e+01   \n",
       "75%    1.058384e+06     8.308000e+06  7.500000e+02 -2.994580e+01   \n",
       "max    1.411179e+06     1.287600e+07  4.370000e+03 -1.823250e+01   \n",
       "\n",
       "           longitud  codigo_cuenca  cantidad_observaciones        caudal  \\\n",
       "count  1.411180e+06   1.411180e+06            1.411180e+06  1.411180e+06   \n",
       "mean  -7.121826e+01   6.255881e+01            1.573983e+04  9.552229e+01   \n",
       "std    1.174363e+00   2.839921e+01            7.545197e+03  2.526337e+02   \n",
       "min   -7.328330e+01   1.000000e+01            8.020000e+02  0.000000e+00   \n",
       "25%   -7.206810e+01   4.300000e+01            1.025500e+04  1.250000e+00   \n",
       "50%   -7.125470e+01   5.700000e+01            1.488900e+04  8.950000e+00   \n",
       "75%   -7.053280e+01   8.300000e+01            2.061100e+04  6.970000e+01   \n",
       "max   -6.814390e+01   1.280000e+02            3.666700e+04  1.580500e+04   \n",
       "\n",
       "           gauge_id  precip_promedio  temp_max_promedio  \n",
       "count  1.411180e+06     1.383413e+06       1.259617e+06  \n",
       "mean   6.279953e+06     1.921009e+00       1.921110e+01  \n",
       "std    2.891695e+06     7.361936e+00       7.307081e+00  \n",
       "min    1.020003e+06     0.000000e+00      -1.905000e+01  \n",
       "25%    4.320001e+06     0.000000e+00       1.370000e+01  \n",
       "50%    5.734001e+06     0.000000e+00       1.908429e+01  \n",
       "75%    8.308000e+06     0.000000e+00       2.520000e+01  \n",
       "max    1.287600e+07     2.586000e+02       4.150000e+01  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['codigo_estacion', 'institucion', 'fuente', 'nombre', 'altura',\n",
       "       'latitud', 'longitud', 'codigo_cuenca', 'nombre_sub_cuenca',\n",
       "       'cantidad_observaciones', 'fecha', 'caudal', 'gauge_id', 'gauge_name',\n",
       "       'precip_promedio', 'temp_max_promedio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Generamos la correlación entre las variables\n",
    "corr = df.corr()\n",
    "\n",
    "# ploteamos el heatmap\n",
    "sns.heatmap(corr, linewidths=.5 , annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOXPLOT DESCRIPTIVO DE CODIGO_CUENCA Y EL CAUDAL CON LA FINALIDAD DE PODER VER LA MEDIA DE LOS DATOS Y LOS DATOS OUTLIERS\n",
    "\n",
    "sns.boxplot(x=\"caudal\", y=\"codigo_cuenca\", data=df,\n",
    "            whis=\"range\", palette=\"vlag\")\n",
    "\n",
    "\n",
    "sns.swarmplot(x=\"caudal\", y=\"codigo_cuenca\", data=df,\n",
    "              size=2, color=\".3\", linewidth=0)\n",
    "\n",
    "\n",
    "ax.xaxis.grid(True)\n",
    "ax.set(ylabel=\"\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOXPLOT DESCRIPTIVO DE CADA CUENCA Y PRECIPITACIÓN PROMEDIO CON LA FINALIDAD DE PODER VER LA MEDIA DE LOS DATOS Y LOS DATOS OUTLIERS\n",
    "\n",
    "sns.boxplot(x=\"precip_promedio\", y=\"codigo_cuenca\", data=df,\n",
    "            whis=\"range\", palette=\"vlag\")\n",
    "\n",
    "\n",
    "sns.swarmplot(x=\"precip_promedio\", y=\"codigo_cuenca\", data=df,\n",
    "              size=2, color=\".3\", linewidth=0)\n",
    "\n",
    "\n",
    "ax.xaxis.grid(True)\n",
    "ax.set(ylabel=\"\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOXPLOT DESCRIPTIVO DE CADA CUENCA Y LA TEMPERATURA PROMEDIO CON LA FINALIDAD DE PODER VER LA MEDIA DE LOS DATOS Y LOS DATOS OUTLIERS\n",
    "\n",
    "sns.boxplot(x=\"temp_max_promedio\", y=\"codigo_cuenca\", data=df,\n",
    "            whis=\"range\", palette=\"vlag\")\n",
    "\n",
    "\n",
    "sns.swarmplot(x=\"temp_max_promedio \", y=\"codigo_cuenca\", data=df,\n",
    "              size=2, color=\".3\", linewidth=0)\n",
    "\n",
    "\n",
    "ax.xaxis.grid(True)\n",
    "ax.set(ylabel=\"\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se procederá a analizar la cantidad total de N/A que posee nuestro dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp_max_promedio         151563\n",
       "precip_promedio            27767\n",
       "gauge_name                     0\n",
       "gauge_id                       0\n",
       "caudal                         0\n",
       "fecha                          0\n",
       "cantidad_observaciones         0\n",
       "nombre_sub_cuenca              0\n",
       "codigo_cuenca                  0\n",
       "longitud                       0\n",
       "latitud                        0\n",
       "altura                         0\n",
       "nombre                         0\n",
       "fuente                         0\n",
       "institucion                    0\n",
       "codigo_estacion                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El total de Celdas con valores Null es: 179330\n"
     ]
    }
   ],
   "source": [
    "Total=df.isnull().sum().sum()\n",
    "print(\"El total de Celdas con valores Null es: \"+str(Total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REEMPLAZO DE LOS MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar el reemplazo de los missing values , se buscará el promedio de los valores y estos serán insertados en los valores que actualmente aparecen como N/A para proceder más adelante a realizar.\n",
    "\n",
    "Si bien , la cantidad de los N/A representa cerca del 12% del total de muestras de este DataFrame, no se procederán a eliminar dado que en su mayoria los demás campos se encuentran poblados salvo 'temp_max_promedio' y 'precip_promedio'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['temp_max_promedio'].fillna(df['temp_max_promedio'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['precip_promedio'].fillna(df['precip_promedio'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp_max_promedio         0\n",
       "precip_promedio           0\n",
       "gauge_name                0\n",
       "gauge_id                  0\n",
       "caudal                    0\n",
       "fecha                     0\n",
       "cantidad_observaciones    0\n",
       "nombre_sub_cuenca         0\n",
       "codigo_cuenca             0\n",
       "longitud                  0\n",
       "latitud                   0\n",
       "altura                    0\n",
       "nombre                    0\n",
       "fuente                    0\n",
       "institucion               0\n",
       "codigo_estacion           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se proceden a realizar las 2 funciones para poder realizar los gráficos según los parámetros solicitados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_plot_una_estacion(codigo_estacion,columna=\"precip_promedio\",fecha_min,fecha_max):\n",
    "    data=df[(df[\"fecha\"]>=fecha_min) & (df[\"fecha\"]<=fecha_max)&(df[\"codigo_estacion\"]==codigo_estacion)]\n",
    "    sns.lineplot(x=df[\"fecha\"], y=columna, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_plot_una_estacion(4540001,precip_promedio,'1960-01-06 00:00:00+00:00','1980-04-09 00:00:00+00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''para esta ocasión por razones de tiempo , se utilizó para ambos plot la libreria de Seaborn siendo el óptimo haber generado\n",
    "los plot con la libreria Matplotlib'''\n",
    "\n",
    "def time_plot_estaciones_varias_columnas(codigo_estacion,columna1=\"caudal\",columna2=\"precip_promedio\",columna3=\"temp_max_promedio\",fecha_min,fecha_max):\n",
    "    corr=df.corr()\n",
    "    data=df[(df[\"fecha\"]>=fecha_min) & (df[\"fecha\"]<=fecha_max)&(df[\"codigo_estacion\"]==codigo_estacion)]\n",
    "    sns.lineplot(x=df[\"fecha\"], y=[\"caudal\",\"precip_promedio\",\"temp_max_promedio\"], data=corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_plot_estaciones_varias_columnas(4540001,,,,'1960-01-06 00:00:00+00:00','1980-04-09 00:00:00+00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREACIÓN 3 VARIABLES NUEVAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['caudal_extremo']=np.where(df['caudal'].rank(pct=True)>.95,1,0)\n",
    "df['temp_extremo']=np.where(df['temp_max_promedio'].rank(pct=True)>.95,1,0)\n",
    "df['precip_extremo']=np.where(df['precip_promedio'].rank(pct=True)>.95,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Les parece razonable esta medida para capturar algo “extremo”? ¿Usarían otra? ¿Cuál? ( Solamente\n",
    "descríbanla, no la codifiquen! Vamos a usar la definición de Spike para esta desafío)\n",
    "\n",
    "Para evitar esto, propondria utilizar los intervalos de confianza para poder definir si corresponde o no a algún tipo distribución Normal y si se encuentran fuera del 95% o el 97,5%, así como ver los valores outliers que podrían estar generando ruido blanco.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANÁLISIS GRÁFICO DE VARIABLES CREADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grupo = df.groupby(['codigo_cuenca', 'caudal_extremo']) \n",
    "grupo.first().sort_values(by=\"codigo_cuenca\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "b =sns.countplot(x=\"codigo_cuenca\", data=df , hue=\"caudal_extremo\")\n",
    "b.axes.set_title(\"Cuencas y Frecuencia de Caudales Extremos\",fontsize=20)\n",
    "b.set_xlabel(\"Cod_Cuenca\",fontsize=10)\n",
    "b.set_ylabel(\"Frecuencia\",fontsize=10)\n",
    "b.tick_params(labelsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = \"fecha\", y = \"caudal_extremo\", data = df, estimator=lambda x: len(x) / len(df) * 100, label = \"Porcentaje Caudal\");\n",
    "sns.lineplot(x = \"fecha\", y = \"temp_extremo\", data = df, estimator = lambda x: len(x) / len(df) * 100, label = \"Porcentaje Temperatura\");\n",
    "sns.lineplot(x = \"fecha\", y = \"precip_extremo\", data = df, estimator = lambda x: len(x) / len(df) * 100, label = \"Porcentaje Precipitaciones\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECCIÓN DE VARIABLES CON FUNCION RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La eliminación de funciones recursivas (RFE), como su título sugiere, elimina funciones de forma recursiva, construye un modelo utilizando los atributos restantes y calcula la precisión del modelo. RFE puede resolver la combinación de atributos que contribuyen a la predicción en la variable (o clase) objetivo. ScikitLearn realiza la mayor parte del trabajo pesado solo importa RFE desde sklearn.feature_selection y pasa cualquier modelo de clasificador al método RFE () con la cantidad de características para seleccionar. Usando la sintaxis familiar de Scikit Learn, se debe llamar al método .fit ()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['codigo_estacion', 'institucion', 'fuente', 'nombre', 'altura',\n",
       "       'latitud', 'longitud', 'codigo_cuenca', 'nombre_sub_cuenca',\n",
       "       'cantidad_observaciones', 'fecha', 'caudal', 'gauge_id', 'gauge_name',\n",
       "       'precip_promedio', 'temp_max_promedio', 'caudal_extremo',\n",
       "       'temp_extremo', 'precip_extremo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data = df.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.loc[:,\"caudal_extremo\"]\n",
    "X=data.loc[:,['codigo_estacion', 'institucion', 'fuente', 'nombre', 'altura','latitud', 'longitud', 'codigo_cuenca', 'nombre_sub_cuenca','cantidad_observaciones', 'fecha', 'caudal', 'gauge_id', 'gauge_name','precip_promedio', 'temp_max_promedio','temp_extremo', 'precip_extremo']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREACIÓN DE MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTINOMIAL NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True)\n",
      "[[353209  89181]\n",
      " [  1298  22002]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89    442390\n",
      "           1       0.20      0.94      0.33     23300\n",
      "\n",
      "    accuracy                           0.81    465690\n",
      "   macro avg       0.60      0.87      0.61    465690\n",
      "weighted avg       0.96      0.81      0.86    465690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "clf_MNB = MultinomialNB(alpha=.01)\n",
    "alpha = [0.001, 0.01, 0.1, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "param_grid = dict(alpha=alpha)\n",
    "grid = GridSearchCV(clf_MNB, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_)\n",
    "y_pred_score_MNB = grid.predict(X_test)\n",
    "pd.Series(y_pred_score_MNB).value_counts()\n",
    "print(confusion_matrix(y_test,y_pred_score_MNB))\n",
    "print(classification_report(y_test,y_pred_score_MNB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-LAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[442352     38]\n",
      " [ 23279     21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97    442390\n",
      "           1       0.36      0.00      0.00     23300\n",
      "\n",
      "    accuracy                           0.95    465690\n",
      "   macro avg       0.65      0.50      0.49    465690\n",
      "weighted avg       0.92      0.95      0.93    465690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train_sc = scaler.transform(X_train)  \n",
    " # apply same transformation to test data\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "NN = MLPClassifier(solver='lbfgs', alpha=1,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "NN.fit(X_train_sc, y_train)\n",
    "y_pred_NN_res=NN.predict(X_test_sc)\n",
    "pd.Series(y_pred_NN_res).value_counts()\n",
    "print(confusion_matrix(y_test,y_pred_NN_res))\n",
    "print(classification_report(y_test,y_pred_NN_res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINE (KERNEL LINEAL Y RADIAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier_rbf = SVC(kernel='rbf')\n",
    "svclassifier_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svclassifier_rbf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred_rbf))\n",
    "print(classification_report(y_test,y_pred_rbf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "classifier = DecisionTreeClassifier() \n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred _DT= svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred_DT))\n",
    "print(classification_report(y_test,y_pred_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGIT (REGRESION LOGÍSTICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    " classifier_log= LogisticRegression(random_state=0,solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)\n",
    "y_pred _LOGIT= classifier_log.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred_LOGIT))\n",
    "print(classification_report(y_test,y_pred_LOGIT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECOMENDACIONES FINALES:\n",
    "\n",
    "Para poder seleccionar el modelo que mejor se ajusta , se debe seleccionar mediante el indicador F1-SCORE ,dado que es el indicador que balancea tanto la PRECISION como el RECALL. Además se sugiere exportar este modelo con la libreria Pickle ó generar API con FLASK para poder realizar pronósticos en tiempo real utilizando los modelos anteriormemte realizados y poder conectar esta bbdd con PSYCOPG2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jordán Efrain Fernández Ruiz\n",
    "\n",
    "Mail: jefernandez3@uc.cl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
